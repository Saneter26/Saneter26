<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Билет 18</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            background-color: #fff;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            max-width: 800px;
            margin: auto;
        }
        h1, h2, h3 {
            color: #0056b3;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
        }
        ol {
            list-style-type: decimal;
            margin-left: 20px;
        }
        b {
            color: #0056b3;
        }
        .formula {
            background-color: #e0eaff;
            padding: 10px;
            border-radius: 5px;
            margin-bottom: 15px;
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Билет 18</h1>

        <h2>Вопрос 1: Формула Байеса.</h2>
        <p><b>Формула Байеса</b> — это ключевой инструмент в теории вероятностей, который позволяет обновить (пересмотреть) вероятность события на основе новой информации или доказательств. Она связывает условные вероятности и является основой для байесовского вывода в статистике.</p>

        <h3>Определение:</h3>
        <p>Пусть $A$ — некоторое событие, а $H_1, H_2, \dots, H_n$ — полная группа несовместных гипотез (то есть, $H_i \cap H_j = \emptyset$ для $i \neq j$, и $\bigcup_{i=1}^{n} H_i = \Omega$, а также $P(H_i) > 0$).</p>
        <p>Если событие $A$ произошло, то вероятность того, что оно произошло в результате гипотезы $H_k$, может быть вычислена по формуле Байеса:</p>
        <div class="formula">
            $$P(H_k|A) = \frac{P(A|H_k) P(H_k)}{P(A)}$$
        </div>
        <p>Где $P(A)$ — полная вероятность события $A$, которая вычисляется по формуле полной вероятности:</p>
        <div class="formula">
            $$P(A) = \sum_{i=1}^{n} P(A|H_i) P(H_i)$$
        </div>
        <p>Таким образом, развернутая формула Байеса выглядит так:</p>
        <div class="formula">
            $$P(H_k|A) = \frac{P(A|H_k) P(H_k)}{\sum_{i=1}^{n} P(A|H_i) P(H_i)}$$
        </div>
        <p>Где:</p>
        <ul>
            <li>$P(H_k|A)$ — <b>апостериорная (послеопытная) вероятность</b> гипотезы $H_k$ при условии, что событие $A$ произошло. Это то, что мы хотим найти.</li>
            <li>$P(H_k)$ — <b>априорная (доопытная) вероятность</b> гипотезы $H_k$. Это наша первоначальная оценка вероятности гипотезы до получения новой информации.</li>
            <li>$P(A|H_k)$ — <b>вероятность правдоподобия</b>, или условная вероятность события $A$ при условии, что гипотеза $H_k$ истинна.</li>
            <li>$P(A)$ — <b>полная вероятность</b> события $A$.</li>
        </ul>

        <h3>Пример применения:</h3>
        <p>Предположим, есть два завода, производящих лампочки:</p>
        <ul>
            <li>Завод 1 производит 70% всех лампочек ($P(H_1) = 0.7$). Доля брака на Заводе 1 составляет 2% ($P(A|H_1) = 0.02$).</li>
            <li>Завод 2 производит 30% всех лампочек ($P(H_2) = 0.3$). Доля брака на Заводе 2 составляет 5% ($P(A|H_2) = 0.05$).</li>
        </ul>
        <p>Вы купили лампочку, и она оказалась бракованной (событие $A$). Какова вероятность, что эта лампочка была произведена на Заводе 1?</p>
        <ol>
            <li><b>Найдём полную вероятность события $A$ (что лампочка бракованная):</b>
                <div class="formula">
                    $$P(A) = P(A|H_1)P(H_1) + P(A|H_2)P(H_2)$$
                    $$P(A) = (0.02 \cdot 0.7) + (0.05 \cdot 0.3) = 0.014 + 0.015 = 0.029$$
                </div>
            </li>
            <li><b>Применим формулу Байеса для $P(H_1|A)$:`</b>
                <div class="formula">
                    $$P(H_1|A) = \frac{P(A|H_1) P(H_1)}{P(A)}$$
                    $$P(H_1|A) = \frac{0.02 \cdot 0.7}{0.029} = \frac{0.014}{0.029} \approx 0.4827$$
                </div>
            </li>
        </ol>
        <p>Таким образом, если лампочка оказалась бракованной, вероятность того, что она была произведена на Заводе 1, составляет примерно 48.27%. Изначально вероятность того, что лампочка с Завода 1, была 70%, но после получения информации о браке эта вероятность снизилась.</p>
        <p>Формула Байеса широко применяется в машинном обучении (например, наивный байесовский классификатор), диагностике, криминалистике, финансовом моделировании и многих других областях для уточнения вероятностей на основе новых данных.</p>

        <h2>Вопрос 2: Ковариация и коэффициент корреляции. Их определение и свойства.</h2>
        <p><b>Ковариация</b> и <b>коэффициент корреляции</b> — это меры, используемые для описания линейной взаимосвязи между двумя случайными величинами. Они показывают, как эти величины изменяются вместе.</p>

        <h3>Ковариация ($Cov(X, Y)$)</h3>
        <p><b>Определение:</b> Ковариация измеряет степень, в которой две случайные величины $X$ и $Y$ изменяются в одном и том же направлении.</p>
        <ul>
            <li>Если $X$ и $Y$ имеют тенденцию увеличиваться или уменьшаться вместе, ковариация будет положительной.</li>
            <li>Если одна величина имеет тенденцию увеличиваться, когда другая уменьшается, ковариация будет отрицательной.</li>
            <li>Если между величинами нет четкой линейной зависимости, ковариация будет близка к нулю.</li>
        </ul>
        <p><b>Формула для дискретных случайных величин:</b></p>
        <div class="formula">
            $$Cov(X, Y) = E[(X - E[X])(Y - E[Y])]$$
            <p>Или в более удобной для расчетов форме:</p>
            $$Cov(X, Y) = E[XY] - E[X]E[Y]$$
        </div>

        <h4>Свойства ковариации:</h4>
        <ol>
            <li><b>Симметричность:</b> $Cov(X, Y) = Cov(Y, X)$.</li>
            <li><b>Ковариация случайной величины с самой собой:</b> $Cov(X, X) = D(X)$ (дисперсия X).</li>
            <li><b>Ковариация с константой:</b> $Cov(X, C) = 0$, где $C$ — константа.</li>
            <li><b>Свойство линейности:</b>
                <ul>
                    <li>$Cov(aX, Y) = a \cdot Cov(X, Y)$</li>
                    <li>$Cov(X, bY) = b \cdot Cov(X, Y)$</li>
                    <li>$Cov(aX, bY) = ab \cdot Cov(X, Y)$</li>
                    <li>$Cov(X + Y, Z) = Cov(X, Z) + Cov(Y, Z)$</li>
                </ul>
            </li>
            <li><b>Ковариация суммы/разности:</b> $Cov(X + Y, Z + W) = Cov(X, Z) + Cov(X, W) + Cov(Y, Z) + Cov(Y, W)$</li>
            <li><b>Дисперсия суммы/разности двух случайных величин:</b>
                <ul>
                    <li>$D(X + Y) = D(X) + D(Y) + 2 Cov(X, Y)$</li>
                    <li>$D(X - Y) = D(X) + D(Y) - 2 Cov(X, Y)$</li>
                </ul>
                <p>Если $X$ и $Y$ независимы, то $Cov(X, Y) = 0$, и тогда $D(X + Y) = D(X) + D(Y)$.</p>
            </li>
        </ol>
        <p><b>Недостатки ковариации:</b> Величина ковариации зависит от масштаба измерения переменных. Например, если измерять рост в сантиметрах вместо метров, ковариация изменится, что затрудняет интерпретацию и сравнение связей между разными парами переменных.</p>

        <h3>Коэффициент корреляции Пирсона ($\rho(X, Y)$ или $r_{XY}$)</h3>
        <p><b>Определение:</b> Коэффициент корреляции (чаще всего используется коэффициент корреляции Пирсона) — это нормированная мера линейной связи между двумя случайными величинами. В отличие от ковариации, он безразмерен и его значения всегда лежат в диапазоне от -1 до +1.</p>
        <p><b>Формула:</b></p>
        <div class="formula">
            $$\rho(X, Y) = \frac{Cov(X, Y)}{\sigma(X) \sigma(Y)}$$
        </div>
        <p>Где $\sigma(X)$ и $\sigma(Y)$ — стандартные отклонения случайных величин $X$ и $Y$ соответственно.</p>

        <h4>Свойства коэффициента корреляции:</h4>
        <ol>
            <li><b>Диапазон значений:</b> $-1 \le \rho(X, Y) \le 1$.</li>
            <li><b>Направление связи:</b>
                <ul>
                    <li>$\rho(X, Y) = 1$: строгая прямая линейная зависимость (с ростом $X$, $Y$ линейно растет).</li>
                    <li>$\rho(X, Y) = -1$: строгая обратная линейная зависимость (с ростом $X$, $Y$ линейно убывает).</li>
                    <li>$\rho(X, Y) = 0$: отсутствие линейной зависимости. Это не означает отсутствие любой зависимости; могут существовать нелинейные связи.</li>
                </ul>
            </li>
            <li><b>Независимость и корреляция:</b> Если $X$ и $Y$ независимы, то $Cov(X, Y) = 0$, и, следовательно, $\rho(X, Y) = 0$. Однако обратное не всегда верно: нулевой коэффициент корреляции не означает независимости, если только речь не идет о нормально распределенных случайных величинах.</li>
            <li><b>Инвариантность к масштабу и сдвигу:</b> Коэффициент корреляции не меняется при умножении переменных на константу или при добавлении константы (при условии, что константа, на которую умножают, не равна нулю). То есть, если $X' = aX + b$ и $Y' = cY + d$ (где $a, c > 0$), то $\rho(X', Y') = \rho(X, Y)$.</li>
        </ol>
        <h4>Применение:</h4>
        <ul>
            <li><b>Исследование взаимосвязей:</b> Определение, например, связи между рекламными расходами и объемом продаж, уровнем образования и доходом, температурой и продажами мороженого.</li>
            <li><b>Регрессионный анализ:</b> Коэффициент корреляции является основой для построения моделей линейной регрессии.</li>
            <li><b>Финансы:</b> Анализ взаимосвязей между ценами акций разных компаний.</li>
        </ul>
        <p>Коэффициент корреляции является более информативной мерой линейной связи, чем ковариация, благодаря своей стандартизации, что позволяет сравнивать силу связей между различными парами переменных независимо от их единиц измерения.</p>
    </div>
</body>
</html>
